{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Exploring Pytorch from http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0000e+00  8.5899e+09  0.0000e+00\n",
       " 8.5899e+09  9.8091e-45  0.0000e+00\n",
       "-6.0290e+07  4.5916e-41 -9.0338e-05\n",
       " 4.5776e-41 -3.1921e-06  4.5776e-41\n",
       " 0.0000e+00  8.5899e+09 -3.5002e-03\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.4164  0.9430  0.3157\n",
       " 0.1858  0.6310  0.5543\n",
       " 0.6195  0.3359  0.9431\n",
       " 0.1693  0.2338  0.0641\n",
       " 0.9217  0.1967  0.0596\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.4811  0.1390  0.2908\n",
       " 0.2304  0.7038  0.4300\n",
       " 0.5630  0.6987  0.9544\n",
       " 0.5612  0.9455  0.2335\n",
       " 0.3034  0.1703  0.2379\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.8975  1.0820  0.6065\n",
       " 0.4162  1.3348  0.9843\n",
       " 1.1825  1.0346  1.8976\n",
       " 0.7305  1.1793  0.2976\n",
       " 1.2251  0.3670  0.2974\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2003  0.1311  0.0918\n",
       " 0.0428  0.4441  0.2383\n",
       " 0.3488  0.2347  0.9002\n",
       " 0.0950  0.2211  0.0150\n",
       " 0.2796  0.0335  0.0142\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.8655  6.7841  1.0854\n",
       " 0.8062  0.8965  1.2890\n",
       " 1.1005  0.4807  0.9882\n",
       " 0.3017  0.2473  0.2746\n",
       " 3.0384  1.1552  0.2504\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x/y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0  0\n",
       " 0  0  0\n",
       " 0  0  0\n",
       " 0  0  0\n",
       " 0  0  0\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)-(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = torch.Tensor(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.8975  1.0820  0.6065\n",
      " 0.4162  1.3348  0.9843\n",
      " 1.1825  1.0346  1.8976\n",
      " 0.7305  1.1793  0.2976\n",
      " 1.2251  0.3670  0.2974\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.add(x,y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.8975  1.0820  0.6065\n",
       " 0.4162  1.3348  0.9843\n",
       " 1.1825  1.0346  1.8976\n",
       " 0.7305  1.1793  0.2976\n",
       " 1.2251  0.3670  0.2974\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.,  2.,  2.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Torch tensor and numpy tensor shares the same memory slot!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.DoubleTensor of size 5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=a*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.DoubleTensor of size 5]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.,  2.,  2.,  2.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.DoubleTensor of size 5]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = b*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "[torch.DoubleTensor of size 5]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.,  2.,  2.,  2.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  3.,  3.,  3.,  3.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(a, 1, out=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Start with next session.\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(2,2), requires_grad=True) # Why requires_grad ? Seems to work anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3  3\n",
       " 3  3\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_2 = Variable(torch.ones(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3  3\n",
       " 3  3\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 ++ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3  3\n",
       " 3  3\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.AddConstantBackward object at 0x1123cf9a8>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok. The backward function is backpropagation for the variable... Pretty cool, not sure just yet when to use it.\n",
    "# the .grad function is the derivative for the backprop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(3), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.7827\n",
       "-1.3807\n",
       " 2.0649\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x *2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.5653\n",
       "-2.7615\n",
       " 4.1298\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " -400.7260\n",
      " -706.9375\n",
      " 1057.2218\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0000e-01\n",
       " 1.0000e+00\n",
       " 1.0000e-04\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.backward(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  51.2000\n",
      " 512.0000\n",
      "   0.0512\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This I do not understand... Need to check this again\n",
    "# the autograd is for back propagation in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next learning module, creating a neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 6.8819e+00 -4.7579e+00 -1.2158e+00  ...   4.0879e+00 -2.2503e+00 -8.5142e-01\n",
       " 3.9815e+00 -1.6308e+00 -2.1050e+00  ...  -5.0594e+00 -1.4911e+00  2.0968e+00\n",
       "-2.1520e+00 -1.3344e+00 -4.8168e+00  ...   2.9763e+00  2.2032e-01  2.8189e+00\n",
       "                ...                   ⋱                   ...                \n",
       " 1.6242e+00  2.4084e+00 -7.3878e-01  ...   1.8846e+00  1.6455e-01  3.0424e+00\n",
       " 2.4516e+00 -4.0589e+00 -2.6730e-01  ...   3.5395e+00 -3.2026e+00  4.0952e+00\n",
       "-2.7240e+00 -1.7947e+00 -1.1664e+00  ...  -1.1634e+00 -2.2169e+00  8.5387e+00\n",
       "[torch.FloatTensor of size 1000x100]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "       ...          ⋱          ...       \n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "[torch.FloatTensor of size 100x10]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "w1.grad.data.zero_()\n",
    "w2.grad.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29134716.0\n",
      "1 24819738.0\n",
      "2 25413290.0\n",
      "3 26990056.0\n",
      "4 26491644.0\n",
      "5 22343314.0\n",
      "6 15840646.0\n",
      "7 9660226.0\n",
      "8 5444684.0\n",
      "9 3075257.25\n",
      "10 1858320.0\n",
      "11 1237979.375\n",
      "12 905452.4375\n",
      "13 710583.875\n",
      "14 583744.3125\n",
      "15 493153.46875\n",
      "16 423731.3125\n",
      "17 367969.15625\n",
      "18 321832.75\n",
      "19 282979.90625\n",
      "20 249900.953125\n",
      "21 221451.796875\n",
      "22 196858.4375\n",
      "23 175489.03125\n",
      "24 156866.453125\n",
      "25 140542.0625\n",
      "26 126166.9765625\n",
      "27 113509.421875\n",
      "28 102317.65625\n",
      "29 92385.6953125\n",
      "30 83543.796875\n",
      "31 75659.421875\n",
      "32 68617.4609375\n",
      "33 62316.40234375\n",
      "34 56661.34375\n",
      "35 51578.96875\n",
      "36 47007.98046875\n",
      "37 42882.48828125\n",
      "38 39145.9375\n",
      "39 35768.0625\n",
      "40 32710.734375\n",
      "41 29940.43359375\n",
      "42 27427.0546875\n",
      "43 25143.28125\n",
      "44 23065.9296875\n",
      "45 21176.001953125\n",
      "46 19453.060546875\n",
      "47 17881.60546875\n",
      "48 16446.40625\n",
      "49 15134.8359375\n",
      "50 13935.2021484375\n",
      "51 12838.4736328125\n",
      "52 11834.00390625\n",
      "53 10913.099609375\n",
      "54 10068.72265625\n",
      "55 9293.5224609375\n",
      "56 8581.6796875\n",
      "57 7927.4775390625\n",
      "58 7326.24755859375\n",
      "59 6773.0146484375\n",
      "60 6263.84521484375\n",
      "61 5795.52734375\n",
      "62 5363.7978515625\n",
      "63 4965.9267578125\n",
      "64 4599.09814453125\n",
      "65 4260.794921875\n",
      "66 3948.532958984375\n",
      "67 3660.1201171875\n",
      "68 3394.2109375\n",
      "69 3148.375732421875\n",
      "70 2920.977783203125\n",
      "71 2710.874267578125\n",
      "72 2516.585693359375\n",
      "73 2336.665771484375\n",
      "74 2170.133056640625\n",
      "75 2015.953857421875\n",
      "76 1873.21630859375\n",
      "77 1740.9222412109375\n",
      "78 1618.3751220703125\n",
      "79 1504.796142578125\n",
      "80 1399.46337890625\n",
      "81 1301.7928466796875\n",
      "82 1211.165771484375\n",
      "83 1127.057373046875\n",
      "84 1049.0245361328125\n",
      "85 976.5614624023438\n",
      "86 909.2876586914062\n",
      "87 846.8173217773438\n",
      "88 788.7831420898438\n",
      "89 734.8727416992188\n",
      "90 684.7709350585938\n",
      "91 638.1975708007812\n",
      "92 594.9000244140625\n",
      "93 554.6365356445312\n",
      "94 517.2047119140625\n",
      "95 482.355224609375\n",
      "96 449.91302490234375\n",
      "97 419.7313537597656\n",
      "98 391.6268310546875\n",
      "99 365.4578857421875\n",
      "100 341.10125732421875\n",
      "101 318.4085693359375\n",
      "102 297.2692565917969\n",
      "103 277.591552734375\n",
      "104 259.240234375\n",
      "105 242.13308715820312\n",
      "106 226.1913299560547\n",
      "107 211.32424926757812\n",
      "108 197.46466064453125\n",
      "109 184.541748046875\n",
      "110 172.48048400878906\n",
      "111 161.22756958007812\n",
      "112 150.73638916015625\n",
      "113 140.93862915039062\n",
      "114 131.8022918701172\n",
      "115 123.27495574951172\n",
      "116 115.30304718017578\n",
      "117 107.86353302001953\n",
      "118 100.91510772705078\n",
      "119 94.42545318603516\n",
      "120 88.36322021484375\n",
      "121 82.69853210449219\n",
      "122 77.4064712524414\n",
      "123 72.46257019042969\n",
      "124 67.84089660644531\n",
      "125 63.518917083740234\n",
      "126 59.480934143066406\n",
      "127 55.70806884765625\n",
      "128 52.17658233642578\n",
      "129 48.87345504760742\n",
      "130 45.78524398803711\n",
      "131 42.89491653442383\n",
      "132 40.19258117675781\n",
      "133 37.66361999511719\n",
      "134 35.29705047607422\n",
      "135 33.08478927612305\n",
      "136 31.012174606323242\n",
      "137 29.07292938232422\n",
      "138 27.256683349609375\n",
      "139 25.55814552307129\n",
      "140 23.965667724609375\n",
      "141 22.47427749633789\n",
      "142 21.078447341918945\n",
      "143 19.77017593383789\n",
      "144 18.54558753967285\n",
      "145 17.39718246459961\n",
      "146 16.32194709777832\n",
      "147 15.314621925354004\n",
      "148 14.370809555053711\n",
      "149 13.485684394836426\n",
      "150 12.656161308288574\n",
      "151 11.878512382507324\n",
      "152 11.150135040283203\n",
      "153 10.466318130493164\n",
      "154 9.825580596923828\n",
      "155 9.2249755859375\n",
      "156 8.661288261413574\n",
      "157 8.132903099060059\n",
      "158 7.637209892272949\n",
      "159 7.171889305114746\n",
      "160 6.735466003417969\n",
      "161 6.326545715332031\n",
      "162 5.942177772521973\n",
      "163 5.581888198852539\n",
      "164 5.243858814239502\n",
      "165 4.926511764526367\n",
      "166 4.628613471984863\n",
      "167 4.349174499511719\n",
      "168 4.086695194244385\n",
      "169 3.840482473373413\n",
      "170 3.609099864959717\n",
      "171 3.3921091556549072\n",
      "172 3.188192129135132\n",
      "173 2.996706962585449\n",
      "174 2.8169219493865967\n",
      "175 2.6480700969696045\n",
      "176 2.489600419998169\n",
      "177 2.3404436111450195\n",
      "178 2.200530767440796\n",
      "179 2.0691614151000977\n",
      "180 1.9457253217697144\n",
      "181 1.8296571969985962\n",
      "182 1.7204782962799072\n",
      "183 1.6181654930114746\n",
      "184 1.521790862083435\n",
      "185 1.431418538093567\n",
      "186 1.3463658094406128\n",
      "187 1.2664377689361572\n",
      "188 1.191422939300537\n",
      "189 1.1207656860351562\n",
      "190 1.05446457862854\n",
      "191 0.9920830726623535\n",
      "192 0.9334962964057922\n",
      "193 0.8781605362892151\n",
      "194 0.8262543678283691\n",
      "195 0.7777133584022522\n",
      "196 0.7317904829978943\n",
      "197 0.6887354850769043\n",
      "198 0.648225724697113\n",
      "199 0.6100910305976868\n",
      "200 0.5742291808128357\n",
      "201 0.540526807308197\n",
      "202 0.508711040019989\n",
      "203 0.4788714051246643\n",
      "204 0.4508129358291626\n",
      "205 0.42439746856689453\n",
      "206 0.399547278881073\n",
      "207 0.376066654920578\n",
      "208 0.35411685705184937\n",
      "209 0.33347249031066895\n",
      "210 0.31398630142211914\n",
      "211 0.2956838309764862\n",
      "212 0.27842977643013\n",
      "213 0.26219797134399414\n",
      "214 0.2469213753938675\n",
      "215 0.23255707323551178\n",
      "216 0.21894295513629913\n",
      "217 0.2062348574399948\n",
      "218 0.19422875344753265\n",
      "219 0.1829327940940857\n",
      "220 0.1723521649837494\n",
      "221 0.1623443067073822\n",
      "222 0.1529613733291626\n",
      "223 0.1440676897764206\n",
      "224 0.13572199642658234\n",
      "225 0.12786288559436798\n",
      "226 0.12051041424274445\n",
      "227 0.11354044079780579\n",
      "228 0.1069423109292984\n",
      "229 0.1007755845785141\n",
      "230 0.0949627086520195\n",
      "231 0.08946625143289566\n",
      "232 0.08431405574083328\n",
      "233 0.07944895327091217\n",
      "234 0.07487911731004715\n",
      "235 0.07056059688329697\n",
      "236 0.06647736579179764\n",
      "237 0.06265836954116821\n",
      "238 0.059044793248176575\n",
      "239 0.055659882724285126\n",
      "240 0.05245940387248993\n",
      "241 0.04945213347673416\n",
      "242 0.04660770669579506\n",
      "243 0.04393307864665985\n",
      "244 0.04141440987586975\n",
      "245 0.03905649855732918\n",
      "246 0.03681475296616554\n",
      "247 0.034709833562374115\n",
      "248 0.03272612392902374\n",
      "249 0.030869197100400925\n",
      "250 0.029109761118888855\n",
      "251 0.02743430808186531\n",
      "252 0.025870084762573242\n",
      "253 0.024384886026382446\n",
      "254 0.022998245432972908\n",
      "255 0.02168702334165573\n",
      "256 0.02044425904750824\n",
      "257 0.019283147528767586\n",
      "258 0.01820489391684532\n",
      "259 0.017173001542687416\n",
      "260 0.016201134771108627\n",
      "261 0.015276705846190453\n",
      "262 0.014426552690565586\n",
      "263 0.013610671274363995\n",
      "264 0.01284512784332037\n",
      "265 0.01212263386696577\n",
      "266 0.011436110362410545\n",
      "267 0.010792802087962627\n",
      "268 0.010193923488259315\n",
      "269 0.00962017197161913\n",
      "270 0.009087204933166504\n",
      "271 0.008587999269366264\n",
      "272 0.008107975125312805\n",
      "273 0.007656622212380171\n",
      "274 0.007230106741189957\n",
      "275 0.006836113519966602\n",
      "276 0.006462514866143465\n",
      "277 0.0061068846844136715\n",
      "278 0.005770438350737095\n",
      "279 0.00545979430899024\n",
      "280 0.005162737797945738\n",
      "281 0.004886572249233723\n",
      "282 0.004620134364813566\n",
      "283 0.004374782554805279\n",
      "284 0.004142208490520716\n",
      "285 0.003922312054783106\n",
      "286 0.003714360296726227\n",
      "287 0.003516266355291009\n",
      "288 0.003334843087941408\n",
      "289 0.0031593525782227516\n",
      "290 0.0029930744785815477\n",
      "291 0.0028421084862202406\n",
      "292 0.002695782110095024\n",
      "293 0.0025618334766477346\n",
      "294 0.0024307239800691605\n",
      "295 0.00230606971308589\n",
      "296 0.002190858591347933\n",
      "297 0.0020817776676267385\n",
      "298 0.001981396460905671\n",
      "299 0.0018841378623619676\n",
      "300 0.0017956418450921774\n",
      "301 0.0017085145227611065\n",
      "302 0.0016274498775601387\n",
      "303 0.0015505620976909995\n",
      "304 0.001478587044402957\n",
      "305 0.00141145009547472\n",
      "306 0.001345060532912612\n",
      "307 0.0012846908066421747\n",
      "308 0.001227748696692288\n",
      "309 0.0011715609580278397\n",
      "310 0.0011194818653166294\n",
      "311 0.0010690330527722836\n",
      "312 0.0010211465414613485\n",
      "313 0.0009778294479474425\n",
      "314 0.0009353188215754926\n",
      "315 0.0008960480918176472\n",
      "316 0.0008582950104027987\n",
      "317 0.0008225811179727316\n",
      "318 0.0007884576916694641\n",
      "319 0.0007561575039289892\n",
      "320 0.0007249150075949728\n",
      "321 0.0006967129302211106\n",
      "322 0.0006697286735288799\n",
      "323 0.0006431385991163552\n",
      "324 0.0006184092490002513\n",
      "325 0.0005950289196334779\n",
      "326 0.000571979209780693\n",
      "327 0.0005508403410203755\n",
      "328 0.0005314992740750313\n",
      "329 0.0005113985389471054\n",
      "330 0.0004919003695249557\n",
      "331 0.0004737709532491863\n",
      "332 0.00045753453741781414\n",
      "333 0.00044079782674089074\n",
      "334 0.0004245357122272253\n",
      "335 0.00040950477705337107\n",
      "336 0.000395455164834857\n",
      "337 0.0003814463852904737\n",
      "338 0.0003671749436762184\n",
      "339 0.00035516644129529595\n",
      "340 0.0003432217345107347\n",
      "341 0.0003317862283438444\n",
      "342 0.0003207306144759059\n",
      "343 0.00031056758598424494\n",
      "344 0.00030028316541574895\n",
      "345 0.0002908807946369052\n",
      "346 0.0002812455059029162\n",
      "347 0.0002730563865043223\n",
      "348 0.0002647373767104\n",
      "349 0.0002564993919804692\n",
      "350 0.0002483302087057382\n",
      "351 0.00024100029258988798\n",
      "352 0.00023379968479275703\n",
      "353 0.00022744409216102213\n",
      "354 0.00021981472673360258\n",
      "355 0.000213298320886679\n",
      "356 0.0002073989307973534\n",
      "357 0.00020174805831629783\n",
      "358 0.00019616284407675266\n",
      "359 0.0001897850597742945\n",
      "360 0.00018476968398317695\n",
      "361 0.00017897698853630573\n",
      "362 0.0001747301866998896\n",
      "363 0.0001701253349892795\n",
      "364 0.00016618594236206263\n",
      "365 0.00016145373228937387\n",
      "366 0.00015691792941652238\n",
      "367 0.00015260756481438875\n",
      "368 0.00014916883083060384\n",
      "369 0.00014553774963133037\n",
      "370 0.000141724813147448\n",
      "371 0.00013822784239891917\n",
      "372 0.00013516242324840277\n",
      "373 0.00013182929251343012\n",
      "374 0.00012858853733632714\n",
      "375 0.00012486986815929413\n",
      "376 0.000122394209029153\n",
      "377 0.00011901680409209803\n",
      "378 0.00011640229058684781\n",
      "379 0.00011363155499566346\n",
      "380 0.00011118626571260393\n",
      "381 0.00010866657248698175\n",
      "382 0.00010623270645737648\n",
      "383 0.00010410595132270828\n",
      "384 0.00010163938713958487\n",
      "385 9.8977550806012e-05\n",
      "386 9.680794028099626e-05\n",
      "387 9.456062252866104e-05\n",
      "388 9.298080112785101e-05\n",
      "389 9.094535926124081e-05\n",
      "390 8.937420352594927e-05\n",
      "391 8.735407027415931e-05\n",
      "392 8.565578900743276e-05\n",
      "393 8.367548434762284e-05\n",
      "394 8.222474571084604e-05\n",
      "395 8.046576840570197e-05\n",
      "396 7.887771789683029e-05\n",
      "397 7.740176079096273e-05\n",
      "398 7.59625036153011e-05\n",
      "399 7.437758904416114e-05\n",
      "400 7.313853711821139e-05\n",
      "401 7.150848250603303e-05\n",
      "402 7.024108344921842e-05\n",
      "403 6.915551057318226e-05\n",
      "404 6.77642019581981e-05\n",
      "405 6.657002813881263e-05\n",
      "406 6.50873189442791e-05\n",
      "407 6.395456148311496e-05\n",
      "408 6.294500053627416e-05\n",
      "409 6.190764543134719e-05\n",
      "410 6.0702685004798695e-05\n",
      "411 5.9734175010817125e-05\n",
      "412 5.844921906827949e-05\n",
      "413 5.7549263146938756e-05\n",
      "414 5.672587940352969e-05\n",
      "415 5.5462100135628134e-05\n",
      "416 5.462253830046393e-05\n",
      "417 5.381758091971278e-05\n",
      "418 5.290855551720597e-05\n",
      "419 5.181465166970156e-05\n",
      "420 5.0997219659620896e-05\n",
      "421 5.035173671785742e-05\n",
      "422 4.944308238918893e-05\n",
      "423 4.8577428970020264e-05\n",
      "424 4.784274642588571e-05\n",
      "425 4.7142559196799994e-05\n",
      "426 4.648184767575003e-05\n",
      "427 4.574395643430762e-05\n",
      "428 4.5169850636739284e-05\n",
      "429 4.454749068827368e-05\n",
      "430 4.3878142605535686e-05\n",
      "431 4.327040005591698e-05\n",
      "432 4.2673975258367136e-05\n",
      "433 4.20198957726825e-05\n",
      "434 4.137808355153538e-05\n",
      "435 4.071282455697656e-05\n",
      "436 4.016318780486472e-05\n",
      "437 3.957203807658516e-05\n",
      "438 3.893305256497115e-05\n",
      "439 3.8363810745067894e-05\n",
      "440 3.7845500628463924e-05\n",
      "441 3.724898851942271e-05\n",
      "442 3.6689027183456346e-05\n",
      "443 3.608161932788789e-05\n",
      "444 3.5494107578415424e-05\n",
      "445 3.5112541809212416e-05\n",
      "446 3.471835589152761e-05\n",
      "447 3.42444000125397e-05\n",
      "448 3.385874879313633e-05\n",
      "449 3.332562846480869e-05\n",
      "450 3.289119922555983e-05\n",
      "451 3.247092172387056e-05\n",
      "452 3.207603731425479e-05\n",
      "453 3.171066418872215e-05\n",
      "454 3.1495190341956913e-05\n",
      "455 3.099706373177469e-05\n",
      "456 3.07239024550654e-05\n",
      "457 3.033617394976318e-05\n",
      "458 3.010483123944141e-05\n",
      "459 2.9545090001192875e-05\n",
      "460 2.9159044061088935e-05\n",
      "461 2.8975429813726805e-05\n",
      "462 2.8686534278676845e-05\n",
      "463 2.837204374372959e-05\n",
      "464 2.8098558686906472e-05\n",
      "465 2.7657624741550535e-05\n",
      "466 2.7374628189136274e-05\n",
      "467 2.713498724915553e-05\n",
      "468 2.682827653188724e-05\n",
      "469 2.6465657356311567e-05\n",
      "470 2.6167530450038612e-05\n",
      "471 2.597661477921065e-05\n",
      "472 2.5810875740717165e-05\n",
      "473 2.5449504391872324e-05\n",
      "474 2.511716593289748e-05\n",
      "475 2.4907929400797002e-05\n",
      "476 2.4632799977553077e-05\n",
      "477 2.441949436615687e-05\n",
      "478 2.4141929316101596e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479 2.3784799850545824e-05\n",
      "480 2.3552682250738144e-05\n",
      "481 2.3207172489492223e-05\n",
      "482 2.3125152438296936e-05\n",
      "483 2.2881149561726488e-05\n",
      "484 2.2613787223235704e-05\n",
      "485 2.2384292606147937e-05\n",
      "486 2.218334520875942e-05\n",
      "487 2.1993526388541795e-05\n",
      "488 2.1896899852436036e-05\n",
      "489 2.1632309653796256e-05\n",
      "490 2.1463965822476894e-05\n",
      "491 2.1285821276251227e-05\n",
      "492 2.1210302293184213e-05\n",
      "493 2.099159246427007e-05\n",
      "494 2.0802737708436325e-05\n",
      "495 2.062277235381771e-05\n",
      "496 2.0285940991016105e-05\n",
      "497 2.0157725884928368e-05\n",
      "498 2.0065017451997846e-05\n",
      "499 1.995091406570282e-05\n"
     ]
    }
   ],
   "source": [
    "y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    print(t, loss.data[0])\n",
    "    \n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = x.mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Variable(torch.ones(3,3)*2)\n",
    "b = Variable(torch.from_numpy(np.array([(1,2,3),(4,5,6),(7,8,9)])).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [3 x 3], m2: [1000 x 100] at /Users/soumith/miniconda2/conda-bld/pytorch_1502000975045/work/torch/lib/TH/generic/THTensorMath.c:1293",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-d506287b7185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmm\u001b[0;34m(self, matrix)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mAddmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [3 x 3], m2: [1000 x 100] at /Users/soumith/miniconda2/conda-bld/pytorch_1502000975045/work/torch/lib/TH/generic/THTensorMath.c:1293"
     ]
    }
   ],
   "source": [
    "a.mm(b).mm(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2  2  2\n",
       " 2  2  2\n",
       " 2  2  2\n",
       "[torch.FloatTensor of size 3x3]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  2  3\n",
       " 4  5  6\n",
       " 7  8  9\n",
       "[torch.FloatTensor of size 3x3]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  2   4   6\n",
       "  8  10  12\n",
       " 14  16  18\n",
       "[torch.FloatTensor of size 3x3]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
